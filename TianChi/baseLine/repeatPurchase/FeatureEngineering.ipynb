{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "velvet-mapping",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T06:55:47.730371Z",
     "start_time": "2021-07-05T06:55:47.717367Z"
    }
   },
   "outputs": [],
   "source": [
    "# 特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "speaking-square",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T06:55:48.899701Z",
     "start_time": "2021-07-05T06:55:47.732351Z"
    }
   },
   "outputs": [],
   "source": [
    "# 导入工具包\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "muslim-mason",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T06:56:13.215792Z",
     "start_time": "2021-07-05T06:55:48.901703Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:\\\\DataSet\\\\Tianchi\\\\repeatPurchase\\\\data_format1\\\\data_format1\\\\train_format1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-3-131198c1b51e>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[0muser_log_path\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34mr'E:\\DataSet\\Tianchi\\repeatPurchase\\data_format1\\data_format1\\user_log_format1.csv'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 13\u001B[1;33m \u001B[0mtrain_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_data_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msep\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m','\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencoding\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'utf-8'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     14\u001B[0m \u001B[0mtest_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtest_data_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msep\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m','\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencoding\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'utf-8'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[0muser_info\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0muser_info_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msep\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m','\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencoding\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'utf-8'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\DEVTOOLS\\Anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[0;32m    608\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    609\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 610\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    611\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    612\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\DEVTOOLS\\Anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    460\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    461\u001B[0m     \u001B[1;31m# Create the parser.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 462\u001B[1;33m     \u001B[0mparser\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    463\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    464\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\DEVTOOLS\\Anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m    817\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"has_index_names\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"has_index_names\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    818\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 819\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    820\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    821\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\DEVTOOLS\\Anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[1;34m(self, engine)\u001B[0m\n\u001B[0;32m   1048\u001B[0m             )\n\u001B[0;32m   1049\u001B[0m         \u001B[1;31m# error: Too many arguments for \"ParserBase\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1050\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mmapping\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# type: ignore[call-arg]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1051\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1052\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_failover_to_python\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\DEVTOOLS\\Anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, src, **kwds)\u001B[0m\n\u001B[0;32m   1865\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1866\u001B[0m         \u001B[1;31m# open handles\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1867\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_open_handles\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1868\u001B[0m         \u001B[1;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhandles\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1869\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mkey\u001B[0m \u001B[1;32min\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;34m\"storage_options\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"encoding\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"memory_map\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"compression\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\DEVTOOLS\\Anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m_open_handles\u001B[1;34m(self, src, kwds)\u001B[0m\n\u001B[0;32m   1366\u001B[0m             \u001B[0mcompression\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"compression\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1367\u001B[0m             \u001B[0mmemory_map\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"memory_map\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1368\u001B[1;33m             \u001B[0mstorage_options\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"storage_options\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1369\u001B[0m         )\n\u001B[0;32m   1370\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\DEVTOOLS\\Anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\common.py\u001B[0m in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    645\u001B[0m                 \u001B[0mencoding\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mioargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mencoding\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    646\u001B[0m                 \u001B[0merrors\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0merrors\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 647\u001B[1;33m                 \u001B[0mnewline\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    648\u001B[0m             )\n\u001B[0;32m    649\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'E:\\\\DataSet\\\\Tianchi\\\\repeatPurchase\\\\data_format1\\\\data_format1\\\\train_format1.csv'"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "\n",
    "# train_data_path = r'E:/DataSet/Tianchi/repeatPurchase/data_format1/train_format1.csv'\n",
    "# test_data_path = r'E:/DataSet/Tianchi/repeatPurchase/data_format1/test_format1.csv'\n",
    "# user_info_path = r'E:/DataSet/Tianchi/repeatPurchase/data_format1/user_info_format1.csv'\n",
    "# user_log_path = r'E:/DataSet/Tianchi/repeatPurchase/data_format1/user_log_format1.csv'\n",
    "\n",
    "train_data_path = r'E:\\DataSet\\Tianchi\\repeatPurchase\\data_format1\\data_format1\\train_format1.csv'\n",
    "test_data_path = r'E:\\DataSet\\Tianchi\\repeatPurchase\\data_format1\\data_format1\\test_format1.csv'\n",
    "user_info_path = r'E:\\DataSet\\Tianchi\\repeatPurchase\\data_format1\\data_format1\\user_info_format1.csv'\n",
    "user_log_path = r'E:\\DataSet\\Tianchi\\repeatPurchase\\data_format1\\data_format1\\user_log_format1.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_data_path, sep=',', encoding='utf-8')\n",
    "test_data = pd.read_csv(test_data_path, sep=',', encoding='utf-8')\n",
    "user_info = pd.read_csv(user_info_path, sep=',', encoding='utf-8')\n",
    "user_log = pd.read_csv(user_log_path, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-nightmare",
   "metadata": {},
   "source": [
    "## 定义统计函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-closure",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T06:56:13.230783Z",
     "start_time": "2021-07-05T06:56:13.217791Z"
    }
   },
   "outputs": [],
   "source": [
    "# 统计数据总数\n",
    "def cnt_(x):\n",
    "    try:\n",
    "        return len(x.split(' '))\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "# 数据唯一值的总数\n",
    "def nunique_(x):\n",
    "    try:\n",
    "        return len(set(x.split(' ')))\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "    \n",
    "# 统计数据最大值\n",
    "def max_(x):\n",
    "    try:\n",
    "        return np.max([float(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "# 统计数据最小值\n",
    "def min_(x):\n",
    "    try:\n",
    "        return np.min([float(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "# 统计数据标准差\n",
    "def std_(x):\n",
    "    try:\n",
    "        return np.std([flaot(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "# 统计数据中topN的函数\n",
    "def most_n(x, n):\n",
    "    try:\n",
    "        # [('709909', 17)] =》[0]：id\n",
    "        return Counter(x.split(' ')).most_common(n)[n-1][0]\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "def most_n_cnt(x, n):\n",
    "    try:\n",
    "        # [('709909', 17)] =》[1]：次数\n",
    "        return Counter(x.split(' ')).most_common(n)[n-1][1]\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-advisory",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T06:56:13.245775Z",
     "start_time": "2021-07-05T06:56:13.232783Z"
    }
   },
   "outputs": [],
   "source": [
    "# 用户特征统计函数\n",
    "# 单个特征的总数\n",
    "def user_cnt(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(cnt_)\n",
    "    return df_data\n",
    "\n",
    "\n",
    "# 单个特征的不重复总数\n",
    "def user_nunique(df_data,single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(nunique_)\n",
    "    return df_data\n",
    "\n",
    "# 单个特征的最大值\n",
    "def user_max(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(max_)\n",
    "    return df_data\n",
    "\n",
    "# 单个特征的最小值\n",
    "def user_min(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(min_)\n",
    "    return df_data\n",
    "\n",
    "# 单个特征的方差\n",
    "def user_std(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(std_)\n",
    "    return df_data\n",
    "\n",
    "# 单个特征的出现次数的值\n",
    "def user_most_n(df_data, single_col, name, n=1):\n",
    "    func = lambda x: most_n(x, n)\n",
    "    df_data[name] = df_data[single_col].apply(func)\n",
    "    return df_data\n",
    "\n",
    "# 单个特征的出现次数的值的总数\n",
    "def user_most_n_cnt(df_data, single_col, name, n=1):\n",
    "    func = lambda x: most_n_cnt(x, n)\n",
    "    df_data[name] = df_data[single_col].apply(func)\n",
    "    return df_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-function",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T06:56:13.260767Z",
     "start_time": "2021-07-05T06:56:13.247775Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义内存压缩方法，（实际上就是用占内存少的数据类型来存储数据）\n",
    "def reduce_mem_usage(df: pd.DataFrame, verbose=True) -> pd.DataFrame:\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if (col_type in numerics):\n",
    "            col_min = df[col].min()\n",
    "            col_max = df[col].max()\n",
    "            if (str(col_type)[:3] == 'int'):\n",
    "                if (col_min > np.iinfo(np.int8).min and col_max < np.iinfo(np.int8).max):\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif (col_min > np.iinfo(np.int16).min and col_max < np.iinfo(np.int16).max):\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif (col_min > np.iinfo(np.int32).min and col_max < np.iinfo(np.int32).max):\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif (col_min > np.iinfo(np.int64).min and col_max < np.iinfo(np.int64).max):\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if col_min > np.finfo(np.float16).min and col_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif col_min > np.finfo(np.float32).min and col_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "                    \n",
    "    end_men = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is : {:.2f} MB'.format(end_men))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_men) / start_mem))\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-aging",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T06:56:17.938095Z",
     "start_time": "2021-07-05T06:56:13.262766Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = reduce_mem_usage(train_data)\n",
    "test_data = reduce_mem_usage(test_data)\n",
    "user_info = reduce_mem_usage(user_info)\n",
    "user_log = reduce_mem_usage(user_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-situation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T06:56:17.953105Z",
     "start_time": "2021-07-05T06:56:17.940093Z"
    }
   },
   "outputs": [],
   "source": [
    "# 查看压缩后的数据信息\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-brunswick",
   "metadata": {},
   "source": [
    "##  数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-machinery",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T06:56:18.192948Z",
     "start_time": "2021-07-05T06:56:17.954085Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 合并用户细腻\n",
    "all_data = train_data.append(test_data)\n",
    "all_data = all_data.merge(user_info, on=['user_id'], how='left')\n",
    "del train_data, test_data, user_info\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-suicide",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T06:56:28.941808Z",
     "start_time": "2021-07-05T06:56:18.194948Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 用户日志数据进行排序\n",
    "user_log = user_log.sort_values(['user_id', 'time_stamp'])\n",
    "# user_log.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-scheme",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T06:56:28.956800Z",
     "start_time": "2021-07-05T06:56:28.942808Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 合并数据 对每个用户的所有字段都进行合并\n",
    "list_join_func = lambda x: ' '.join([str(i) for i in x])\n",
    "agg_dict = {\n",
    "    'item_id': list_join_func,\n",
    "    'cat_id': list_join_func,\n",
    "    'seller_id': list_join_func,\n",
    "    'brand_id': list_join_func,\n",
    "    'time_stamp': list_join_func,\n",
    "    'action_type': list_join_func,\n",
    "}\n",
    "rename_dict = {\n",
    "    'item_id': 'item_path',\n",
    "    'cat_id': 'cat_path',\n",
    "    'seller_id': 'seller_path',\n",
    "    'brand_id': 'brand_path',\n",
    "    'time_stamp': 'time_stamp_path',\n",
    "    'action_type': 'action_type_path',\n",
    "}\n",
    "\n",
    "\n",
    "def merge_list(df_ID, join_columns, df_data, agg_dict, rename_dict):\n",
    "    df_data = df_data.groupby(join_columns).agg(agg_dict).reset_index().rename(columns=rename_dict)\n",
    "    df_ID = df_ID.merge(df_data, on=join_columns, how='left')\n",
    "    return df_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-lunch",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T06:59:03.713481Z",
     "start_time": "2021-07-05T06:56:28.957799Z"
    }
   },
   "outputs": [],
   "source": [
    "all_data = merge_list(all_data, 'user_id', user_log, agg_dict, rename_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-costa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T06:59:03.818387Z",
     "start_time": "2021-07-05T06:59:03.715446Z"
    }
   },
   "outputs": [],
   "source": [
    "del user_log\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liquid-arrangement",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T07:02:26.913321Z",
     "start_time": "2021-07-05T06:59:03.820386Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " 提取基本特征\n",
    "\"\"\"\n",
    "\n",
    "# all_data_test = all_data.head(2000)\n",
    "all_data_test = all_data\n",
    "\n",
    "# 总次数\n",
    "all_data_test = user_cnt(all_data_test, 'seller_path', 'user_cnt')\n",
    "# 不同店铺个数\n",
    "all_data_test = user_nunique(all_data_test, 'seller_path', 'seller_nunique')\n",
    "# 不同品类个数\n",
    "all_data_test = user_nunique(all_data_test, 'cat_path', 'cat_nunique')\n",
    "# 不同品牌个数\n",
    "all_data_test = user_nunique(all_data_test, 'brand_path', 'brand_nunique')\n",
    "# 不同商品个数\n",
    "all_data_test = user_nunique(all_data_test, 'item_path', 'item_nunique')\n",
    "# 活跃天数\n",
    "all_data_test = user_nunique(all_data_test, 'time_stamp_path', 'time_stamp_nunique')\n",
    "# 不同用户行为种类\n",
    "all_data_test = user_nunique(all_data_test, 'action_type_path', 'action_type_nunique')\n",
    "# 最晚时间\n",
    "all_data_test = user_max(all_data_test, 'action_type_path', 'time_stamp_max')\n",
    "# 最早时间\n",
    "all_data_test = user_min(all_data_test, 'action_type_path', 'time_stamp_min')\n",
    "# 活跃天数方差\n",
    "all_data_test = user_std(all_data_test, 'action_type_path', 'time_stamp_std')\n",
    "# 最早时间和最晚相差天数\n",
    "all_data_test['time_stamp_range'] = all_data_test['time_stamp_max'] - all_data_test['time_stamp_min']\n",
    "# 用户最喜欢的店铺\n",
    "all_data_test = user_most_n(all_data_test, 'seller_path', 'seller_most_1', n=1)\n",
    "# 最喜欢的类目\n",
    "all_data_test = user_most_n(all_data_test, 'cat_path', 'cat_most_1', n=1)\n",
    "# 最喜欢的品牌\n",
    "all_data_test = user_most_n(all_data_test, 'brand_path', 'brand_most_1', n=1)\n",
    "# 最常见的行为动作\n",
    "all_data_test = user_most_n(all_data_test, 'action_type_path', 'action_type_1', n=1)\n",
    "# 用户最喜欢店铺的行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'seller_path', 'seller_most_1_cnt', n=1)\n",
    "# 用户最喜欢类目的行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'cat_path', 'cat_most_1_cnt', n=1)\n",
    "# 用户最喜欢品牌的行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'brand_path', 'brand_most_1_cnt', n=1)\n",
    "# 用户最常见行为的次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'action_type_path', 'action_type_path_1_cnt', n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-reducing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T02:37:02.080996Z",
     "start_time": "2021-07-05T02:36:35.409165Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-container",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T07:02:26.928282Z",
     "start_time": "2021-07-05T07:02:26.915289Z"
    }
   },
   "outputs": [],
   "source": [
    "# 对用户特征进行统计 对点击、加购、购买、收藏分开统计\n",
    "def col_cnt_(df_data, columns_list, action_type):\n",
    "    try:\n",
    "        data_dict = {}\n",
    "        col_list = copy.deepcopy(columns_list)\n",
    "        if action_type != None:\n",
    "            col_list += ['action_type_path']\n",
    "        for col in col_list:\n",
    "            # 一个特征下的值拆分成列表\n",
    "            data_dict[col] = df_data[col].split(' ')\n",
    "\n",
    "        path_len = len(data_dict[col])\n",
    "        data_out = []\n",
    "        for i_ in range(path_len):\n",
    "            data_txt = ''\n",
    "            for col_ in columns_list:\n",
    "                if data_dict['action_type_path'][i_] == action_type:\n",
    "                    data_txt += '_' + data_dict[col_][i_]\n",
    "            data_out.append(data_txt)\n",
    "        return len(data_out)\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def col_nunique_(df_data, columns_list, action_type):\n",
    "    try:\n",
    "        data_dict = {}\n",
    "        col_list = copy.deepcopy(columns_list)\n",
    "        if action_type != None:\n",
    "            col_list += ['action_type_path']\n",
    "        for col in col_list:\n",
    "            # 一个特征下的值拆分成列表\n",
    "            data_dict[col] = df_data[col].split(' ')\n",
    "        path_len = len(data_dict[col])\n",
    "        data_out = []\n",
    "        for i_ in range(path_len):\n",
    "            data_txt = ''\n",
    "            for col_ in columns_list:\n",
    "                if data_dict['action_type_path'][i_] == action_type:\n",
    "                    data_txt += '_' + data_dict[col_][i_]\n",
    "            data_out.append(data_txt)\n",
    "        return len(set(data_out))\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def user_col_cnt(df_data, columns_list, action_type, name):\n",
    "    df_data[name] = df_data.apply(lambda x: col_cnt_(x, columns_list, action_type), axis=1)\n",
    "    return df_data\n",
    "\n",
    "def user_col_nunique(df_data, columns_list, action_type, name):\n",
    "    df_data[name] = df_data.apply(lambda x: col_nunique_(x, columns_list, action_type), axis=1)\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-forwarding",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T07:09:06.442097Z",
     "start_time": "2021-07-05T07:05:28.524561Z"
    }
   },
   "outputs": [],
   "source": [
    "# 总点击次数\n",
    "all_data_test = all_data_test.copy()\n",
    "all_data_test = user_col_cnt(all_data_test, ['seller_path'], '0', 'user_cnt_0')\n",
    "# 加入购物车次数\n",
    "all_data_test = user_col_cnt(all_data_test, ['seller_path'], '1', 'user_cnt_1')\n",
    "# 购买次数\n",
    "all_data_test = user_col_cnt(all_data_test, ['seller_path'], '2', 'user_cnt_2')\n",
    "# 收藏次数\n",
    "all_data_test = user_col_cnt(all_data_test, ['seller_path'], '3', 'user_cnt_3')\n",
    "# 不同店铺数\n",
    "all_data_test = user_col_nunique(all_data_test, ['seller_path', 'item_path'], '0', 'seller_nuique_0')\n",
    "# all_data_test[[ 'user_cnt_0','user_cnt_1', 'user_cnt_2', 'user_cnt_3', 'seller_nuique_0']].head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-addition",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T07:09:06.457088Z",
     "start_time": "2021-07-05T07:09:06.444067Z"
    }
   },
   "outputs": [],
   "source": [
    "all_data_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-triangle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T07:10:59.387543Z",
     "start_time": "2021-07-05T07:09:06.459081Z"
    }
   },
   "outputs": [],
   "source": [
    "## 利用CountVector和TF_ID提取特征\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from scipy import sparse\n",
    "\n",
    "tfidfVec = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS,\n",
    "                           ngram_range=(1, 1),\n",
    "                           max_features=100)\n",
    "columns_list = ['seller_path']\n",
    "for i, col in enumerate(columns_list):\n",
    "    tfidfVec.fit(all_data_test[col])\n",
    "    data_ = tfidfVec.transform(all_data_test[col])\n",
    "    if i == 0:\n",
    "        data_cat = data_\n",
    "    else:\n",
    "        data_cat = sparse.hstack((data_cat, data_))\n",
    "        \n",
    "# 特征命名和特征合并\n",
    "df_tfidf = pd.DataFrame(data_cat.toarray())\n",
    "df_tfidf.columns = ['tfidf_' + str(i) for i in df_tfidf.columns]\n",
    "all_data_test = pd.concat([all_data_test, df_tfidf], axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-canyon",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T07:14:05.267355Z",
     "start_time": "2021-07-05T07:10:59.389543Z"
    }
   },
   "outputs": [],
   "source": [
    "#  嵌入特征\n",
    "import gensim\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = gensim.models.Word2Vec(all_data_test['seller_path'].apply(lambda x: x.split(' ')),\n",
    "                               vector_size=100,\n",
    "                               window=5,\n",
    "                               min_count=5,\n",
    "                               workers=4)\n",
    "model.save('product2vec.model')\n",
    "# model = gensim.models.Word2Vec.load('product2Vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9b2756",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  gensim.models.Word2Vec?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-pipeline",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T07:15:35.950547Z",
     "start_time": "2021-07-05T07:14:05.269383Z"
    }
   },
   "outputs": [],
   "source": [
    "def mean_w2v_(x, model, size=100):\n",
    "    try:\n",
    "        i = 0\n",
    "        for word in x.split(' '):\n",
    "            if word in model.wv.vocab:\n",
    "                i += 1\n",
    "                if i == 1:\n",
    "                    vec = np.zeros(size)\n",
    "                vev += model.wv[word]\n",
    "        return vec / 1\n",
    "    except:\n",
    "        return np.zeros(size)\n",
    "\n",
    "def get_mean_w2v(df_data, columns, model, size):\n",
    "    data_array = []\n",
    "    for index, row in df_data.iterrows():\n",
    "        w2v = mean_w2v_(row[columns], model, size)\n",
    "        data_array.append(w2v)\n",
    "    return pd.DataFrame(data_array)\n",
    "\n",
    "df_embeeding = get_mean_w2v(all_data_test, 'seller_path', model, 100)\n",
    "df_embeeding.columns = ['embeeding_' + str(i) for i in df_embeeding.columns]\n",
    "all_data_test  = pd.concat([all_data_test, df_embeeding], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-quality",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T07:17:36.512074Z",
     "start_time": "2021-07-05T07:15:35.951547Z"
    }
   },
   "outputs": [],
   "source": [
    "all_data_test.to_csv('all_data_test.csv', sep='\\t', index=True, header=True)\n",
    "# all_data_test = pd.read_csv('all_data_test.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-missile",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T07:17:36.676972Z",
     "start_time": "2021-07-05T07:17:36.514067Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.cross_validation import KFold\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from scipy import sparse\n",
    "import xgboost\n",
    "import lightgbm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-coaching",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T07:17:36.766916Z",
     "start_time": "2021-07-05T07:17:36.678970Z"
    }
   },
   "outputs": [],
   "source": [
    "def stacking_clf(clf, train_x, train_y, test_x, clf_name, kf, label_split=None):\n",
    "    train = np.zeros((train_x.shape[0], 1))\n",
    "    test = np.zeros((test_x.shape[0], 1))\n",
    "    test_pre = np.empty((folds, test_x.shape[0], 1))\n",
    "    cv_scores = []\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(train_x, label_split)):\n",
    "        tr_x = train_x[train_index]\n",
    "        tr_y = train_y[train_index]\n",
    "        te_x = train_x[test_index]\n",
    "        te_y = train_y[test_index]\n",
    "        if clf_name in ['rf', 'ada', 'gb', 'et', 'lrj', 'knn', 'gnb']:\n",
    "            clf.fit(tr_x, tr_y)\n",
    "            pre = clf.predict_proba(te_x)\n",
    "            train[test_index] = pre[:, 0].reshape(-1, 1)\n",
    "            test_pre[i, :] = clf.predict_proba(test_x)[:, 0].reshape(-1, 1)\n",
    "            cv_scores.append(log_loss(te_y, pre[:, 0].reshape(-1, 1)))\n",
    "        elif clf_name in ['xgb']:\n",
    "            train_matrix = clf.DMatrix(tr_x, label=tr_y, missing=-1)\n",
    "            test_matrix = clf.DMatrix(te_x, label=te_y, missing=-1)\n",
    "            z = clf.DMatrix(test_x, label=None, missing=-1)\n",
    "            params = {\n",
    "                'booster': 'gbtree',\n",
    "                'objective': 'multi:softprob',\n",
    "                'eval_metric': 'mlogloss',\n",
    "                'gamma': 1,\n",
    "                'min_child_weight': 1.5,\n",
    "                'max_depth': 5,\n",
    "                'lambda': 10,\n",
    "                'subsample': 0.7,\n",
    "                'colsample_bytree': 0.7,\n",
    "                'colsample_bylevel': 0.7,\n",
    "                'eta': 0.03,\n",
    "                'tree_method': 'exact',\n",
    "                'seed': 2021,\n",
    "                'num_class': 2\n",
    "            }\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            watchlist = [(train_matrix, 'train'), (test_matrix, 'eval')]\n",
    "            if test_matrix:\n",
    "                model = clf.train(params,\n",
    "                                  train_matrix,\n",
    "                                  num_boost_round=num_round,\n",
    "                                  evals=watchlist,\n",
    "                                  early_stopping_rounds=early_stopping_rounds)\n",
    "                pre = model.predict(test_matrix, ntree_limit=model.best_ntree_limit)\n",
    "                train[test_index] = pre[:, 0].reshape(-1, 1)\n",
    "                test_pre[i, :] = model.predict(z, ntree_limit=model.best_ntree_limit)[:, 0].reshape(-1, 1)\n",
    "                cv_scores.append(log_loss(te_y, pre[:, 0].reshape(-1, 1)))\n",
    "        elif clf_name in ['lgb']:\n",
    "            train_matrix = clf.Dataset(tr_x, label=tr_y)\n",
    "            test_matrix = clf.Dataset(te_x, label=te_y)\n",
    "            params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                # 'objective': 'dart',\n",
    "                'objective': 'multiclass',\n",
    "                'metric': 'multi_logloss',\n",
    "                'min_child_weight': 1.5,\n",
    "                'num_leaves': 2 ** 5,\n",
    "                'lambda_l2': 10,\n",
    "                'subsample': 0.7,\n",
    "                'colsample_bytree': 0.7,\n",
    "                'colsample_bylevel': 0.7,\n",
    "                'learning_rate': 0.03,\n",
    "                'tree_method': 'exact',\n",
    "                'seed': 2021,\n",
    "                'num_class': 2,\n",
    "                'silent': True\n",
    "            }\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            if test_matrix:\n",
    "                model = clf.train(params,\n",
    "                                  train_matrix,\n",
    "                                  num_round,\n",
    "                                  valid_sets=test_matrix,\n",
    "                                  early_stopping_rounds=early_stopping_rounds)\n",
    "                pre = model.predict(te_x, num_iteration=model.best_iteration)\n",
    "                train[test_index] = pre[:, 0].reshape(-1, 1)\n",
    "                test_pre[i, :] = model.predict(test_x,\n",
    "                                               num_iteration=model.best_iteration)[:, 0].reshape(-1, 1)\n",
    "                cv_scores.append(log_loss(te_y, pre[:, 0].reshape(-1, 1)))\n",
    "        else:\n",
    "            raise IOError(\"Please add new clf.\")\n",
    "        print('%s now score is :' % clf_name, cv_scores)\n",
    "    test[:] = test_pre.mean(axis=0)\n",
    "    print('%s_score_list:' % clf_name, cv_scores)\n",
    "    print('%s_score_mean:' % clf_name, np.mean(cv_scores))\n",
    "    return train.reshape(-1, 1), test.reshape(-1, 1)\n",
    "\n",
    "\n",
    "def rf_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    randomforest = RandomForestClassifier(n_estimators=1200,\n",
    "                                          max_depth=20,\n",
    "                                          n_jobs=-1,\n",
    "                                          random_state=2021,\n",
    "                                          max_features='auto',\n",
    "                                          verbose=1)\n",
    "    rf_train, rf_test = stacking_clf(randomforest,\n",
    "                                     x_train,\n",
    "                                     y_train,\n",
    "                                     x_valid,\n",
    "                                     'rf',\n",
    "                                     kf,\n",
    "                                     label_split=label_split)\n",
    "    return rf_train, rf_test, 'rf'\n",
    "\n",
    "\n",
    "def ada_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    adaboost = AdaBoostClassifier(n_estimators=50,\n",
    "                                  random_state=2021,\n",
    "                                  learning_rate=0.01)\n",
    "    ada_train, ada_test = stacking_clf(adaboost,\n",
    "                                       x_train,\n",
    "                                       y_train,\n",
    "                                       x_valid,\n",
    "                                       'ada',\n",
    "                                       kf,\n",
    "                                       label_split=label_split)\n",
    "    return ada_train, ada_test, 'ada'\n",
    "\n",
    "\n",
    "def gb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    gbdt = GradientBoostingClassifier(learning_rate=0.14,\n",
    "                                      n_estimators=100,\n",
    "                                      subsample=0.8,\n",
    "                                      random_state=2017,\n",
    "                                      max_depth=5,\n",
    "                                      verbose=1)\n",
    "    gbdt_train, gbdt_test = stacking_clf(gbdt,\n",
    "                                         x_train,\n",
    "                                         y_train,\n",
    "                                         x_valid,\n",
    "                                         'gb',\n",
    "                                         kf,\n",
    "                                         label_split=label_split)\n",
    "    return gbdt_train, gbdt_test, 'gb'\n",
    "\n",
    "\n",
    "def et_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    extratree = ExtraTreesClassifier(n_estimators=1200,\n",
    "                                     max_depth=35,\n",
    "                                     max_features='auto',\n",
    "                                     n_jobs=-1,\n",
    "                                     random_state=2021,\n",
    "                                     verbose=1)\n",
    "    et_train, et_test = stacking_clf(extratree,\n",
    "                                     x_train,\n",
    "                                     y_train,\n",
    "                                     x_valid,\n",
    "                                     'et',\n",
    "                                     kf,\n",
    "                                     label_split=label_split)\n",
    "    return et_train, et_test, 'et'\n",
    "\n",
    "\n",
    "def xgb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    xgb_train, xgb_test = stacking_clf(xgboost,\n",
    "                                       x_train,\n",
    "                                       y_train,\n",
    "                                       x_valid,\n",
    "                                       'xgb',\n",
    "                                       kf,\n",
    "                                       label_split=label_split)\n",
    "    return xgb_train, xgb_test, 'xgb'\n",
    "\n",
    "\n",
    "def lgb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    lgb_train, lgb_test = stacking_clf(lightgbm,\n",
    "                                       x_train,\n",
    "                                       y_train,\n",
    "                                       x_valid,\n",
    "                                       'lgb',\n",
    "                                       kf,\n",
    "                                       label_split=label_split)\n",
    "    return lgb_train, lgb_test, 'lgb'\n",
    "\n",
    "\n",
    "def gnb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    gnb = GaussianNB()\n",
    "    gnb_train, gnb_test = stacking_clf(gnb,\n",
    "                                       x_train,\n",
    "                                       y_train,\n",
    "                                       x_valid,\n",
    "                                       'gnb',\n",
    "                                       kf,\n",
    "                                       label_split=label_split)\n",
    "    return gnb_train, gnb_test, 'gnb'\n",
    "\n",
    "\n",
    "def lr_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    logistic_regression = LogisticRegression(n_jobs=-1,\n",
    "                                             random_state=2021,\n",
    "                                             C=0.1,\n",
    "                                             max_iter=200)\n",
    "    lr_train, lr_test = stacking_clf(logistic_regression,\n",
    "                                     x_train,\n",
    "                                     y_train,\n",
    "                                     x_valid,\n",
    "                                     'lr',\n",
    "                                     kf,\n",
    "                                     label_split=label_split)\n",
    "    return lr_train, lr_test, 'gnb'\n",
    "\n",
    "\n",
    "def knn_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    keighbors = KNeighborsClassifier(n_neighbors=200, n_jobs=-1)\n",
    "    knn_train, knn_test = stacking_clf(keighbors,\n",
    "                                       x_train,\n",
    "                                       y_train,\n",
    "                                       x_valid,\n",
    "                                       'knn',\n",
    "                                       kf,\n",
    "                                       label_split=label_split)\n",
    "    return knn_train, knn_test, 'knn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-hybrid",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T07:04:41.666308Z",
     "start_time": "2021-07-05T06:55:47.750Z"
    }
   },
   "outputs": [],
   "source": [
    "features_columns = [c for c in all_data_test.columns if c not in ['label','prob', 'seller_path', 'cat_path',\n",
    "                                                                  'brand_path', 'action_type_path', 'item_path',\n",
    "                                                                  'time_stamp_path']]\n",
    "\n",
    "x_train = all_data_test[~all_data_test['label'].isna()][features_columns].values\n",
    "y_train = all_data_test[~all_data_test['label'].isna()]['label'].values\n",
    "x_valid = all_data_test[all_data_test['label'].isna()][features_columns].values\n",
    "\n",
    "def get_matrix(data):\n",
    "    where_are_nan = np.isnan(data)\n",
    "    where_are_inf = np.isinf(data)\n",
    "    data[where_are_nan] = 0\n",
    "    data[where_are_inf] = 0\n",
    "    return data\n",
    "\n",
    "x_train = np.float_(get_matrix(np.float_(x_train)))\n",
    "y_train = np.int_(y_train)\n",
    "x_valid = x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-treasury",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T07:04:41.667308Z",
     "start_time": "2021-07-05T06:55:47.752Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "folds=5\n",
    "seed=1\n",
    "kf = KFold(n_splits=folds, shuffle=True, random_state=0)\n",
    "clf_list = [lgb_clf, xgb_clf]\n",
    "clf_list_col = ['lgb_clf', 'xgb_clf']\n",
    "clf_list = clf_list\n",
    "column_list = []\n",
    "train_data_list = []\n",
    "test_data_list = []\n",
    "for clf in clf_list:\n",
    "    train_data, test_data, clf_name = clf(x_train, y_train, x_valid, kf, label_split=None)\n",
    "    train_data_list.append(train_data)\n",
    "    test_data_list.append(test_data)\n",
    "    \n",
    "    \n",
    "# xgb now score is : [2.720361767960214, 2.72451861386343, 2.71626851918042, 2.7200282624835457, 2.7110121479770943]\n",
    "# xgb_score_list: [2.720361767960214, 2.72451861386343, 2.71626851918042, 2.7200282624835457, 2.7110121479770943]\n",
    "# xgb_score_mean: 2.718437862292941\n",
    "    \n",
    "train_stacking = np.concatenate(train_data_list, axis=1)\n",
    "test_stacking = np.concatenate(test_data_list, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-kazakhstan",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 原始特征与Stacking特征合并\n",
    "train = pd.DataFrame(np.concatenate([x_train, train_stacking], axis=1))\n",
    "test = np.concatenate([x_valid, test_stacking], axis=1)\n",
    "\n",
    "df_train_all = pd.DataFrame(train)\n",
    "df_train_all.columns = features_columns + clf_list_col\n",
    "df_test_all = pd.DataFrame(test)\n",
    "df_test_all.columns = features_columns + clf_list_col\n",
    "\n",
    "df_train_all['user_id'] = all_data_test[~all_data_test['label'].isna()]['user_id']\n",
    "df_test_all['user_id'] = all_data_test[all_data_test['label'].isna()]['user_id']\n",
    "df_train_all['label'] = all_data_test[~all_data_test['label'].isna()]['label']\n",
    "\n",
    "df_train_all.to_csv('train_all.csv', header=True, index=False)\n",
    "df_test_all.to_csv('test_all.csv', header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-artist",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-oliver",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}