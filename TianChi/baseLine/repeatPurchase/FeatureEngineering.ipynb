{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "velvet-mapping",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T06:55:47.730371Z",
     "start_time": "2021-07-05T06:55:47.717367Z"
    }
   },
   "outputs": [],
   "source": [
    "# 特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "speaking-square",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T06:55:48.899701Z",
     "start_time": "2021-07-05T06:55:47.732351Z"
    }
   },
   "outputs": [],
   "source": [
    "# 导入工具包\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "muslim-mason",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T06:56:13.215792Z",
     "start_time": "2021-07-05T06:55:48.901703Z"
    }
   },
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "\n",
    "train_data_path = r'E:/DataSet/Tianchi/repeatPurchase/data_format1/train_format1.csv'\n",
    "test_data_path = r'E:/DataSet/Tianchi/repeatPurchase/data_format1/test_format1.csv'\n",
    "user_info_path = r'E:/DataSet/Tianchi/repeatPurchase/data_format1/user_info_format1.csv'\n",
    "user_log_path = r'E:/DataSet/Tianchi/repeatPurchase/data_format1/user_log_format1.csv'\n",
    "\n",
    "# train_data_path = r'E:\\DataSet\\Tianchi\\repeatPurchase\\data_format1\\data_format1\\train_format1.csv'\n",
    "# test_data_path = r'E:\\DataSet\\Tianchi\\repeatPurchase\\data_format1\\data_format1\\test_format1.csv'\n",
    "# user_info_path = r'E:\\DataSet\\Tianchi\\repeatPurchase\\data_format1\\data_format1\\user_info_format1.csv'\n",
    "# user_log_path = r'E:\\DataSet\\Tianchi\\repeatPurchase\\data_format1\\data_format1\\user_log_format1.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_data_path, sep=',', encoding='utf-8')\n",
    "test_data = pd.read_csv(test_data_path, sep=',', encoding='utf-8')\n",
    "user_info = pd.read_csv(user_info_path, sep=',', encoding='utf-8')\n",
    "user_log = pd.read_csv(user_log_path, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-nightmare",
   "metadata": {},
   "source": [
    "## 定义统计函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "worthy-closure",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T06:56:13.230783Z",
     "start_time": "2021-07-05T06:56:13.217791Z"
    }
   },
   "outputs": [],
   "source": [
    "# 统计数据总数\n",
    "def cnt_(x):\n",
    "    try:\n",
    "        return len(x.split(' '))\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "# 数据唯一值的总数\n",
    "def nunique_(x):\n",
    "    try:\n",
    "        return len(set(x.split(' ')))\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "    \n",
    "# 统计数据最大值\n",
    "def max_(x):\n",
    "    try:\n",
    "        return np.max([float(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "# 统计数据最小值\n",
    "def min_(x):\n",
    "    try:\n",
    "        return np.min([float(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "# 统计数据标准差\n",
    "def std_(x):\n",
    "    try:\n",
    "        return np.std([flaot(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "# 统计数据中topN的函数\n",
    "def most_n(x, n):\n",
    "    try:\n",
    "        # [('709909', 17)] =》[0]：id\n",
    "        return Counter(x.split(' ')).most_common(n)[n-1][0]\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "def most_n_cnt(x, n):\n",
    "    try:\n",
    "        # [('709909', 17)] =》[1]：次数\n",
    "        return Counter(x.split(' ')).most_common(n)[n-1][1]\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pleasant-advisory",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T06:56:13.245775Z",
     "start_time": "2021-07-05T06:56:13.232783Z"
    }
   },
   "outputs": [],
   "source": [
    "# 用户特征统计函数\n",
    "# 单个特征的总数\n",
    "def user_cnt(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(cnt_)\n",
    "    return df_data\n",
    "\n",
    "\n",
    "# 单个特征的不重复总数\n",
    "def user_nunique(df_data,single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(nunique_)\n",
    "    return df_data\n",
    "\n",
    "# 单个特征的最大值\n",
    "def user_max(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(max_)\n",
    "    return df_data\n",
    "\n",
    "# 单个特征的最小值\n",
    "def user_min(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(min_)\n",
    "    return df_data\n",
    "\n",
    "# 单个特征的方差\n",
    "def user_std(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(std_)\n",
    "    return df_data\n",
    "\n",
    "# 单个特征的出现次数的值\n",
    "def user_most_n(df_data, single_col, name, n=1):\n",
    "    func = lambda x: most_n(x, n)\n",
    "    df_data[name] = df_data[single_col].apply(func)\n",
    "    return df_data\n",
    "\n",
    "# 单个特征的出现次数的值的总数\n",
    "def user_most_n_cnt(df_data, single_col, name, n=1):\n",
    "    func = lambda x: most_n_cnt(x, n)\n",
    "    df_data[name] = df_data[single_col].apply(func)\n",
    "    return df_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "egyptian-function",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T06:56:13.260767Z",
     "start_time": "2021-07-05T06:56:13.247775Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义内存压缩方法，（实际上就是用占内存少的数据类型来存储数据）\n",
    "def reduce_mem_usage(df: pd.DataFrame, verbose=True) -> pd.DataFrame:\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if (col_type in numerics):\n",
    "            col_min = df[col].min()\n",
    "            col_max = df[col].max()\n",
    "            if (str(col_type)[:3] == 'int'):\n",
    "                if (col_min > np.iinfo(np.int8).min and col_max < np.iinfo(np.int8).max):\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif (col_min > np.iinfo(np.int16).min and col_max < np.iinfo(np.int16).max):\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif (col_min > np.iinfo(np.int32).min and col_max < np.iinfo(np.int32).max):\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif (col_min > np.iinfo(np.int64).min and col_max < np.iinfo(np.int64).max):\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if col_min > np.finfo(np.float16).min and col_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif col_min > np.finfo(np.float32).min and col_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "                    \n",
    "    end_men = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is : {:.2f} MB'.format(end_men))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_men) / start_mem))\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "african-aging",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T06:56:17.938095Z",
     "start_time": "2021-07-05T06:56:13.262766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is : 1.74 MB\n",
      "Decreased by 70.8%\n",
      "Memory usage after optimization is : 3.49 MB\n",
      "Decreased by 41.7%\n",
      "Memory usage after optimization is : 3.24 MB\n",
      "Decreased by 66.7%\n",
      "Memory usage after optimization is : 890.48 MB\n",
      "Decreased by 69.6%\n"
     ]
    }
   ],
   "source": [
    "train_data = reduce_mem_usage(train_data)\n",
    "test_data = reduce_mem_usage(test_data)\n",
    "user_info = reduce_mem_usage(user_info)\n",
    "user_log = reduce_mem_usage(user_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "christian-situation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T06:56:17.953105Z",
     "start_time": "2021-07-05T06:56:17.940093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 260864 entries, 0 to 260863\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count   Dtype\n",
      "---  ------       --------------   -----\n",
      " 0   user_id      260864 non-null  int32\n",
      " 1   merchant_id  260864 non-null  int16\n",
      " 2   label        260864 non-null  int8 \n",
      "dtypes: int16(1), int32(1), int8(1)\n",
      "memory usage: 1.7 MB\n"
     ]
    }
   ],
   "source": [
    "# 查看压缩后的数据信息\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-brunswick",
   "metadata": {},
   "source": [
    "##  数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "spoken-machinery",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T06:56:18.192948Z",
     "start_time": "2021-07-05T06:56:17.954085Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 合并用户细腻\n",
    "all_data = train_data.append(test_data)\n",
    "all_data = all_data.merge(user_info, on=['user_id'], how='left')\n",
    "del train_data, test_data, user_info\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "alone-suicide",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T06:56:28.941808Z",
     "start_time": "2021-07-05T06:56:18.194948Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 用户日志数据进行排序\n",
    "user_log = user_log.sort_values(['user_id', 'time_stamp'])\n",
    "# user_log.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "removed-scheme",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T06:56:28.956800Z",
     "start_time": "2021-07-05T06:56:28.942808Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 合并数据 对每个用户的所有字段都进行合并\n",
    "list_join_func = lambda x: ' '.join([str(i) for i in x])\n",
    "agg_dict = {\n",
    "    'item_id': list_join_func,\n",
    "    'cat_id': list_join_func,\n",
    "    'seller_id': list_join_func,\n",
    "    'brand_id': list_join_func,\n",
    "    'time_stamp': list_join_func,\n",
    "    'action_type': list_join_func,\n",
    "}\n",
    "rename_dict = {\n",
    "    'item_id': 'item_path',\n",
    "    'cat_id': 'cat_path',\n",
    "    'seller_id': 'seller_path',\n",
    "    'brand_id': 'brand_path',\n",
    "    'time_stamp': 'time_stamp_path',\n",
    "    'action_type': 'action_type_path',\n",
    "}\n",
    "\n",
    "\n",
    "def merge_list(df_ID, join_columns, df_data, agg_dict, rename_dict):\n",
    "    df_data = df_data.groupby(join_columns).agg(agg_dict).reset_index().rename(columns=rename_dict)\n",
    "    df_ID = df_ID.merge(df_data, on=join_columns, how='left')\n",
    "    return df_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fabulous-lunch",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T06:59:03.713481Z",
     "start_time": "2021-07-05T06:56:28.957799Z"
    }
   },
   "outputs": [],
   "source": [
    "all_data = merge_list(all_data, 'user_id', user_log, agg_dict, rename_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dress-costa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T06:59:03.818387Z",
     "start_time": "2021-07-05T06:59:03.715446Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del user_log\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "liquid-arrangement",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T07:02:26.913321Z",
     "start_time": "2021-07-05T06:59:03.820386Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " 提取基本特征\n",
    "\"\"\"\n",
    "\n",
    "# all_data_test = all_data.head(2000)\n",
    "all_data_test = all_data\n",
    "\n",
    "# 总次数\n",
    "all_data_test = user_cnt(all_data_test, 'seller_path', 'user_cnt')\n",
    "# 不同店铺个数\n",
    "all_data_test = user_nunique(all_data_test, 'seller_path', 'seller_nunique')\n",
    "# 不同品类个数\n",
    "all_data_test = user_nunique(all_data_test, 'cat_path', 'cat_nunique')\n",
    "# 不同品牌个数\n",
    "all_data_test = user_nunique(all_data_test, 'brand_path', 'brand_nunique')\n",
    "# 不同商品个数\n",
    "all_data_test = user_nunique(all_data_test, 'item_path', 'item_nunique')\n",
    "# 活跃天数\n",
    "all_data_test = user_nunique(all_data_test, 'time_stamp_path', 'time_stamp_nunique')\n",
    "# 不同用户行为种类\n",
    "all_data_test = user_nunique(all_data_test, 'action_type_path', 'action_type_nunique')\n",
    "# 最晚时间\n",
    "all_data_test = user_max(all_data_test, 'action_type_path', 'time_stamp_max')\n",
    "# 最早时间\n",
    "all_data_test = user_min(all_data_test, 'action_type_path', 'time_stamp_min')\n",
    "# 活跃天数方差\n",
    "all_data_test = user_std(all_data_test, 'action_type_path', 'time_stamp_std')\n",
    "# 最早时间和最晚相差天数\n",
    "all_data_test['time_stamp_range'] = all_data_test['time_stamp_max'] - all_data_test['time_stamp_min']\n",
    "# 用户最喜欢的店铺\n",
    "all_data_test = user_most_n(all_data_test, 'seller_path', 'seller_most_1', n=1)\n",
    "# 最喜欢的类目\n",
    "all_data_test = user_most_n(all_data_test, 'cat_path', 'cat_most_1', n=1)\n",
    "# 最喜欢的品牌\n",
    "all_data_test = user_most_n(all_data_test, 'brand_path', 'brand_most_1', n=1)\n",
    "# 最常见的行为动作\n",
    "all_data_test = user_most_n(all_data_test, 'action_type_path', 'action_type_1', n=1)\n",
    "# 用户最喜欢店铺的行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'seller_path', 'seller_most_1_cnt', n=1)\n",
    "# 用户最喜欢类目的行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'cat_path', 'cat_most_1_cnt', n=1)\n",
    "# 用户最喜欢品牌的行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'brand_path', 'brand_most_1_cnt', n=1)\n",
    "# 用户最常见行为的次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'action_type_path', 'action_type_path_1_cnt', n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-reducing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T02:37:02.080996Z",
     "start_time": "2021-07-05T02:36:35.409165Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "viral-container",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T07:02:26.928282Z",
     "start_time": "2021-07-05T07:02:26.915289Z"
    }
   },
   "outputs": [],
   "source": [
    "# 对用户特征进行统计 对点击、加购、购买、收藏分开统计\n",
    "def col_cnt_(df_data, columns_list, action_type):\n",
    "    try:\n",
    "        data_dict = {}\n",
    "        col_list = copy.deepcopy(columns_list)\n",
    "        if action_type != None:\n",
    "            col_list += ['action_type_path']\n",
    "        for col in col_list:\n",
    "            # 一个特征下的值拆分成列表\n",
    "            data_dict[col] = df_data[col].split(' ')\n",
    "\n",
    "        path_len = len(data_dict[col])\n",
    "        data_out = []\n",
    "        for i_ in range(path_len):\n",
    "            data_txt = ''\n",
    "            for col_ in columns_list:\n",
    "                if data_dict['action_type_path'][i_] == action_type:\n",
    "                    data_txt += '_' + data_dict[col_][i_]\n",
    "            data_out.append(data_txt)\n",
    "        return len(data_out)\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def col_nunique_(df_data, columns_list, action_type):\n",
    "    try:\n",
    "        data_dict = {}\n",
    "        col_list = copy.deepcopy(columns_list)\n",
    "        if action_type != None:\n",
    "            col_list += ['action_type_path']\n",
    "        for col in col_list:\n",
    "            # 一个特征下的值拆分成列表\n",
    "            data_dict[col] = df_data[col].split(' ')\n",
    "        path_len = len(data_dict[col])\n",
    "        data_out = []\n",
    "        for i_ in range(path_len):\n",
    "            data_txt = ''\n",
    "            for col_ in columns_list:\n",
    "                if data_dict['action_type_path'][i_] == action_type:\n",
    "                    data_txt += '_' + data_dict[col_][i_]\n",
    "            data_out.append(data_txt)\n",
    "        return len(set(data_out))\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def user_col_cnt(df_data, columns_list, action_type, name):\n",
    "    df_data[name] = df_data.apply(lambda x: col_cnt_(x, columns_list, action_type), axis=1)\n",
    "    return df_data\n",
    "\n",
    "def user_col_nunique(df_data, columns_list, action_type, name):\n",
    "    df_data[name] = df_data.apply(lambda x: col_nunique_(x, columns_list, action_type), axis=1)\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eleven-forwarding",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T07:09:06.442097Z",
     "start_time": "2021-07-05T07:05:28.524561Z"
    }
   },
   "outputs": [],
   "source": [
    "# 总点击次数\n",
    "all_data_test = all_data_test.copy()\n",
    "all_data_test = user_col_cnt(all_data_test, ['seller_path'], '0', 'user_cnt_0')\n",
    "# 加入购物车次数\n",
    "all_data_test = user_col_cnt(all_data_test, ['seller_path'], '1', 'user_cnt_1')\n",
    "# 购买次数\n",
    "all_data_test = user_col_cnt(all_data_test, ['seller_path'], '2', 'user_cnt_2')\n",
    "# 收藏次数\n",
    "all_data_test = user_col_cnt(all_data_test, ['seller_path'], '3', 'user_cnt_3')\n",
    "# 不同店铺数\n",
    "all_data_test = user_col_nunique(all_data_test, ['seller_path', 'item_path'], '0', 'seller_nuique_0')\n",
    "# all_data_test[[ 'user_cnt_0','user_cnt_1', 'user_cnt_2', 'user_cnt_3', 'seller_nuique_0']].head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "secure-addition",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T07:09:06.457088Z",
     "start_time": "2021-07-05T07:09:06.444067Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'merchant_id', 'label', 'prob', 'age_range', 'gender',\n",
       "       'item_path', 'cat_path', 'seller_path', 'brand_path', 'time_stamp_path',\n",
       "       'action_type_path', 'user_cnt', 'seller_nunique', 'cat_nunique',\n",
       "       'brand_nunique', 'item_nunique', 'time_stamp_nunique',\n",
       "       'action_type_nunique', 'time_stamp_max', 'time_stamp_min',\n",
       "       'time_stamp_std', 'time_stamp_range', 'seller_most_1', 'cat_most_1',\n",
       "       'brand_most_1', 'action_type_1', 'seller_most_1_cnt', 'cat_most_1_cnt',\n",
       "       'brand_most_1_cnt', 'action_type_path_1_cnt', 'user_cnt_0',\n",
       "       'user_cnt_1', 'user_cnt_2', 'user_cnt_3', 'seller_nuique_0'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "subject-triangle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T07:10:59.387543Z",
     "start_time": "2021-07-05T07:09:06.459081Z"
    }
   },
   "outputs": [],
   "source": [
    "## 利用CountVector和TF_ID提取特征\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from scipy import sparse\n",
    "\n",
    "tfidfVec = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS,\n",
    "                           ngram_range=(1, 1),\n",
    "                           max_features=100)\n",
    "columns_list = ['seller_path']\n",
    "for i, col in enumerate(columns_list):\n",
    "    tfidfVec.fit(all_data_test[col])\n",
    "    data_ = tfidfVec.transform(all_data_test[col])\n",
    "    if i == 0:\n",
    "        data_cat = data_\n",
    "    else:\n",
    "        data_cat = sparse.hstack((data_cat, data_))\n",
    "        \n",
    "# 特征命名和特征合并\n",
    "df_tfidf = pd.DataFrame(data_cat.toarray())\n",
    "df_tfidf.columns = ['tfidf_' + str(i) for i in df_tfidf.columns]\n",
    "all_data_test = pd.concat([all_data_test, df_tfidf], axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "legislative-canyon",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T07:14:05.267355Z",
     "start_time": "2021-07-05T07:10:59.389543Z"
    }
   },
   "outputs": [],
   "source": [
    "#  嵌入特征\n",
    "import gensim\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = gensim.models.Word2Vec(all_data_test['seller_path'].apply(lambda x: x.split(' ')),\n",
    "                               size=100,\n",
    "                               window=5,\n",
    "                               min_count=5,\n",
    "                               workers=4)\n",
    "model.save('product2vec.model')\n",
    "# model = gensim.models.Word2Vec.load('product2Vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "thousand-pipeline",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T07:15:35.950547Z",
     "start_time": "2021-07-05T07:14:05.269383Z"
    }
   },
   "outputs": [],
   "source": [
    "def mean_w2v_(x, model, size=100):\n",
    "    try:\n",
    "        i = 0\n",
    "        for word in x.split(' '):\n",
    "            if word in model.wv.vocab:\n",
    "                i += 1\n",
    "                if i == 1:\n",
    "                    vec = np.zeros(size)\n",
    "                vev += model.wv[word]\n",
    "        return vec / 1\n",
    "    except:\n",
    "        return np.zeros(size)\n",
    "\n",
    "def get_mean_w2v(df_data, columns, model, size):\n",
    "    data_array = []\n",
    "    for index, row in df_data.iterrows():\n",
    "        w2v = mean_w2v_(row[columns], model, size)\n",
    "        data_array.append(w2v)\n",
    "    return pd.DataFrame(data_array)\n",
    "\n",
    "df_embeeding = get_mean_w2v(all_data_test, 'seller_path', model, 100)\n",
    "df_embeeding.columns = ['embeeding_' + str(i) for i in df_embeeding.columns]\n",
    "all_data_test  = pd.concat([all_data_test, df_embeeding], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "satisfactory-quality",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T07:17:36.512074Z",
     "start_time": "2021-07-05T07:15:35.951547Z"
    }
   },
   "outputs": [],
   "source": [
    "all_data_test.to_csv('all_data_test.csv', sep='\\t', index=True, header=True)\n",
    "# all_data_test = pd.read_csv('all_data_test.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "prospective-missile",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T07:17:36.676972Z",
     "start_time": "2021-07-05T07:17:36.514067Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.cross_validation import KFold\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from scipy import sparse\n",
    "import xgboost\n",
    "import lightgbm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "seventh-coaching",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T07:17:36.766916Z",
     "start_time": "2021-07-05T07:17:36.678970Z"
    }
   },
   "outputs": [],
   "source": [
    "def stacking_clf(clf, train_x, train_y, test_x, clf_name, kf, label_split=None):\n",
    "    train = np.zeros((train_x.shape[0], 1))\n",
    "    test = np.zeros((test_x.shape[0], 1))\n",
    "    test_pre = np.empty((folds, test_x.shape[0], 1))\n",
    "    cv_scores = []\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(train_x, label_split)):\n",
    "        tr_x = train_x[train_index]\n",
    "        tr_y = train_y[train_index]\n",
    "        te_x = train_x[test_index]\n",
    "        te_y = train_y[test_index]\n",
    "        if clf_name in ['rf', 'ada', 'gb', 'et', 'lrj', 'knn', 'gnb']:\n",
    "            clf.fit(tr_x, tr_y)\n",
    "            pre = clf.predict_proba(te_x)\n",
    "            train[test_index] = pre[:, 0].reshape(-1, 1)\n",
    "            test_pre[i, :] = clf.predict_proba(test_x)[:, 0].reshape(-1, 1)\n",
    "            cv_scores.append(log_loss(te_y, pre[:, 0].reshape(-1, 1)))\n",
    "        elif clf_name in ['xgb']:\n",
    "            train_matrix = clf.DMatrix(tr_x, label=tr_y, missing=1)\n",
    "            test_matrix = clf.DMatrix(te_x, label=te_y, missing=-1)\n",
    "            z = clf.DMatrix(test_x, label=te_y, missing=-1)\n",
    "            params = {\n",
    "                'booster': 'gbtree',\n",
    "                'objective': 'multi:softprob',\n",
    "                'eval_metric': 'mlogloss',\n",
    "                'gamma': 1,\n",
    "                'min_child_weight': 1.5,\n",
    "                'max_depth': 5,\n",
    "                'labmda': 10,\n",
    "                'subsample': 0.7,\n",
    "                'colsample_bytree': 0.7,\n",
    "                'colsample_bylevel': 0.7,\n",
    "                'eta': 0.03,\n",
    "                'tree_method': 'exact',\n",
    "                'seed': 2017,\n",
    "                'num_class': 2\n",
    "            }\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            watchlist = [(train_matrix, 'train'), (test_matrix, 'eval')]\n",
    "            if test_matrix:\n",
    "                model = clf.train(params,\n",
    "                                  train_matrix,\n",
    "                                  num_boost_round=num_round,\n",
    "                                  evals=watchlist,\n",
    "                                  early_stopping_rounds=early_stopping_rounds)\n",
    "                pre = model.predict(test_matrix,\n",
    "                                    ntree_limit=model.best_ntree_limit)\n",
    "                train[test_index] = pre[:, 0].reshape(-1, 1)\n",
    "                test_pre[i, :] = model.predict(z, ntree_limit=model.ntree_limit)[:, 0].reshape(-1, 1)\n",
    "                cv_scores.append(log_loss(te_y, pre[:, 0].reshape(-1, 1)))\n",
    "        elif clf_name in ['lgb']:\n",
    "            train_matrix = clf.Dataset(tr_x, label=tr_y)\n",
    "            test_matrix = clf.Dataset(te_x, label=te_y)\n",
    "            params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'dart',\n",
    "                'metric': 'multi_logloss',\n",
    "                'min_child_weight': 1.5,\n",
    "                'num_leaves': 2**5,\n",
    "                'lambda_l2': 10,\n",
    "                'subsample': 0.7,\n",
    "                'colsample_bytree': 0.7,\n",
    "                'colsample_bylevel': 0.7,\n",
    "                'learning_rate': 0.03,\n",
    "                'tree_method': 'exact',\n",
    "                'seed': 2021,\n",
    "                'num_class': 2,\n",
    "                'silent': True\n",
    "            }\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            if test_matrix:\n",
    "                model = clf.train(params,\n",
    "                                  train_matrix,\n",
    "                                  num_round,\n",
    "                                  valid_sets=test_matrix,\n",
    "                                  early_stopping_rounds=early_stopping_rounds)\n",
    "                pre = model.predict(te_x, num_iteration=model.best_iteration)\n",
    "                train[test_index] = pre[:, 0].reshape(-1, 1)\n",
    "                test_pre[i, :] = model.predict(test_x,\n",
    "                                               num_iteration=model.best_iteration)[:, 0].reshape(-1, 1)\n",
    "                cv_socres.append(log_loss(te_y, pre[:, 0].reshape(-1, 1)))\n",
    "        else:\n",
    "            raise IOError(\"Please add new clf.\")\n",
    "        print('%s now score is :' % clf_name, cv_scores)\n",
    "    test[:] = test_pre.mean(axis=0)\n",
    "    print('%s_score_list:' % clf_name, cv_scores)\n",
    "    print('%s_score_mean:' % clf_name, np.mean(cv_socres))\n",
    "    return train.reshape(-1, 1), test.shape(-1, 1)\n",
    "\n",
    "\n",
    "def rf_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    randomforest = RandomForestClassifier(n_estimators=1200,\n",
    "                                          max_depth=20,\n",
    "                                          n_jobs=-1,\n",
    "                                          random_state=2021,\n",
    "                                          max_features='auto',\n",
    "                                          verbose=1)\n",
    "    rf_train, rf_test = stacking_clf(randomforest,\n",
    "                                     x_train,\n",
    "                                     y_train,\n",
    "                                     x_valid,\n",
    "                                     'rf',\n",
    "                                     kf,\n",
    "                                     label_split=label_split)\n",
    "    return rf_train, tf_test, 'rf'\n",
    "\n",
    "\n",
    "def ada_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    adaboost = AdaBoostClassifier(n_estimators=50,\n",
    "                                  random_state=2021,\n",
    "                                  learning_rate=0.01)\n",
    "    ada_train, ada_test = stacking_clf(adaboost,\n",
    "                                       x_train,\n",
    "                                       y_train,\n",
    "                                       x_valid,\n",
    "                                       'ada',\n",
    "                                       kf,\n",
    "                                       label_split=label_split)\n",
    "    return ada_train, ada_test, 'ada'\n",
    "\n",
    "\n",
    "def gb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    gbdt = GradientBoostingClassifier(learning_rate=0.14,\n",
    "                                      n_estimators=100,\n",
    "                                      subsample=0.8,\n",
    "                                      random_state=2017,\n",
    "                                      max_depth=5,\n",
    "                                      verbose=1)\n",
    "    gbdt_train, gbdt_test = stacking_clf(gbdt,\n",
    "                                       x_train,\n",
    "                                       y_train,\n",
    "                                       x_valid,\n",
    "                                       'gb',\n",
    "                                       kf,\n",
    "                                       label_split=label_split)\n",
    "    return gbdt_train, gbdt_test, 'gb'\n",
    "\n",
    "\n",
    "def et_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    extratree = ExtraTreesClassifier(n_estimators=1200,\n",
    "                                     max_depth=35,\n",
    "                                     max_features='auto',\n",
    "                                     n_jobs=-1,\n",
    "                                     random_state=2021,\n",
    "                                     verbose=1)\n",
    "    et_train, et_test = stacking_clf(extratree,\n",
    "                                       x_train,\n",
    "                                       y_train,\n",
    "                                       x_valid,\n",
    "                                       'et',\n",
    "                                       kf,\n",
    "                                       label_split=label_split)\n",
    "    return et_train, et_test, 'et'\n",
    "\n",
    "def xgb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    xgb_train, xgb_test = stacking_clf(xgboost,\n",
    "                                       x_train,\n",
    "                                       y_train,\n",
    "                                       x_valid,\n",
    "                                       'xgb',\n",
    "                                       kf,\n",
    "                                       label_split=label_split)\n",
    "    return xgb_train, xgb_test, 'xgb'\n",
    "\n",
    "\n",
    "\n",
    "def lgb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    lgb_train, lgb_test = stacking_clf(lightgbm,\n",
    "                                       x_train,\n",
    "                                       y_train,\n",
    "                                       x_valid,\n",
    "                                       'lgb',\n",
    "                                       kf,\n",
    "                                       label_split=label_split)\n",
    "    return lgb_train, lgb_test, 'lgb'\n",
    "\n",
    "\n",
    "\n",
    "def gnb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    gnb = GaussianNB()\n",
    "    gnb_train, gnb_test = stacking_clf(gnb,\n",
    "                                       x_train,\n",
    "                                       y_train,\n",
    "                                       x_valid,\n",
    "                                       'gnb',\n",
    "                                       kf,\n",
    "                                       label_split=label_split)\n",
    "    return gnb_train, gnb_test, 'gnb'\n",
    "\n",
    "\n",
    "def lr_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    logistic_regression = LogisticRegression(n_jobs=-1,\n",
    "                                             random_state=2021,\n",
    "                                             C=0.1,\n",
    "                                             max_iter=200)\n",
    "    lr_train, lr_test = stacking_clf(logistic_regression,\n",
    "                                       x_train,\n",
    "                                       y_train,\n",
    "                                       x_valid,\n",
    "                                       'lr',\n",
    "                                       kf,\n",
    "                                       label_split=label_split)\n",
    "    return lr_train, lr_test, 'gnb'\n",
    "\n",
    "\n",
    "def knn_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    keighbors = KNeighborsClassifier(n_neighbors=200, n_jobs=-1)\n",
    "    knn_train, knn_test = stacking_clf(keighbors,\n",
    "                                       x_train,\n",
    "                                       y_train,\n",
    "                                       x_valid,\n",
    "                                       'knn',\n",
    "                                       kf,\n",
    "                                       label_split=label_split)\n",
    "    return knn_train, knn_test, 'gnb'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-hybrid",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T07:04:41.666308Z",
     "start_time": "2021-07-05T06:55:47.750Z"
    }
   },
   "outputs": [],
   "source": [
    "# features_columns = [c for c in all_data_test.columns if c not in ['label','prob', 'seller_path', 'cat_path', \n",
    "#                                                                   'brand_path', 'action_type_path', 'item_path', \n",
    "#                                                                   'time_stamp_path']]\n",
    "\n",
    "# x_train = all_data_test[~all_data_test['label'].isna()][features_columns].values\n",
    "# y_train = all_data_test[~all_data_test['label'].isna()]['label'].values\n",
    "# x_valid = all_data_test[all_data_test['label'].isna()][features_columns].values\n",
    "\n",
    "# def get_matrix(data):\n",
    "#     where_are_nan = np.isnan(data)\n",
    "#     where_are_inf = np.isinf(data)\n",
    "#     data[where_are_nan] = 0\n",
    "#     data[where_are_inf] = 0 \n",
    "#     return data\n",
    "\n",
    "# x_train = np.float_(get_matrix(np.float_(x_train)))\n",
    "# y_train = np.int_(y_train)\n",
    "# x_valid = x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-treasury",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T07:04:41.667308Z",
     "start_time": "2021-07-05T06:55:47.752Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold, KFold\n",
    "# folds=5\n",
    "# seed=1\n",
    "# kf = KFold(n_splits=folds, shuffle=True, random_state=0)\n",
    "# clf_list = [lgb_clf, xgb_clf]\n",
    "# clf_list_col = ['lgb_clf', 'xgb_clf']\n",
    "# clf_list = clf_list\n",
    "# column_list = []\n",
    "# column_list = []\n",
    "# train_data_list = []\n",
    "# test_data_list = []\n",
    "# for clf in clf_list:\n",
    "#     train_data, test_data, clf_name = clf(x_train, y_train, x_valid, kf, label_split=None)\n",
    "#     train_data_list.append(train_data)\n",
    "#     test_data_list.append(test_data)\n",
    "    \n",
    "# train_stacking = np.concatenate(train_data_list, axis=1)\n",
    "# test_stacking = np.concatenate(test_data_list, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-artist",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-oliver",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
